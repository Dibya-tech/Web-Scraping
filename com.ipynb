{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"â€“disable-extensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wildfire Column headers\n",
    "table_0_headers = ['Event_id','Episode','Event_type','Impact_url']\n",
    "table_1_headers = ['Event_id','Episode','Countries','Start_date_last_detected','Duration','People_affected','Burned_area','Event_summary']\n",
    "table_2_headers = ['Event_id','Episode','ID','Alert_Color','GDACS_Score','Population_Affected','Burned_Area','Last_Update','GWIS']\n",
    "table_3_headers = ['Event_id','Episode','Radius','Population']\n",
    "table_4_headers = ['Event_id','Episode','Region_province','Country','Population']\n",
    "table_5_headers = ['Event_id','Episode','Name','Region_Province','Country','City_class','Population','Distance']\n",
    "table_6_headers = ['Event_id','Episode','Name','IATA_Code','Elevation_in_m','Usage','Runway_type','IFR','Runway_Length_in_ft','Distance']\n",
    "table_7_headers = ['Event_id','Episode','Name','LOCODE','Country','Distance']\n",
    "table_8_headers = ['Event_id','Episode','Reservoir','Dam_Name','River','Year','Distance']\n",
    "table_9_headers = ['Event_id','Episode','Name','Country','Reactor','Distance']\n",
    "# Function to extract table data based on XPath and the number of columns required\n",
    "def extract_table_data(driver, event_id, episode, xpath, col_limit):\n",
    "    table_data = []\n",
    "    if xpath == '//*[@id=\"alert_summary_left\"]/table/tbody':\n",
    "        try:\n",
    "            temp_tbl=[]\n",
    "            table = driver.find_element(By.XPATH, xpath)\n",
    "            rows = table.find_elements(By.XPATH, './/tr')\n",
    "            event_summary_text = driver.find_element(By.XPATH, '//*[@class=\"p_summary\"][1]').text\n",
    "            for row in rows:\n",
    "                columns = row.find_elements(By.XPATH, './/td')\n",
    "                row_data = [column.text for column in columns]\n",
    "                if len(row_data)>=1:\n",
    "                    temp_tbl.append(row_data)\n",
    "            table_data = [[event_id, episode,*[temp_tbl[i][1] for i in range(1,col_limit)]\n",
    "                        ,event_summary_text]]\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Event summary not founds or table with XPath '{xpath}' not found.\")\n",
    "        return table_data\n",
    "    else:\n",
    "        try:\n",
    "            table = driver.find_element(By.XPATH, xpath)\n",
    "            rows = table.find_elements(By.XPATH, './/tr')\n",
    "            for row in rows:\n",
    "                columns = row.find_elements(By.XPATH, './/td' if col_limit else './/th | .//td')\n",
    "                row_data = [col.text.strip() for col in columns]\n",
    "                if len(row_data) >= 1:\n",
    "                    table_data.append([event_id, episode] + row_data[:col_limit if col_limit else len(row_data)])\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Table with XPath '{xpath}' not found.\")\n",
    "        return table_data\n",
    "\n",
    "# Main function\n",
    "def wildfire_csv(html_file):\n",
    "    # File and event details\n",
    "    file_name = html_file.split('/')[-1].split('.')[0]\n",
    "    event_id, episode, event_type = file_name.split('_')[0:3]\n",
    "    print(event_id, episode, event_type)\n",
    "\n",
    "    # Initialize the WebDriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(html_file)\n",
    "\n",
    "\n",
    "    # Define table configurations (XPaths and column limits)\n",
    "    table_configs = [\n",
    "        ('//*[@id=\"alert_summary_left\"]/table/tbody', 6),\n",
    "        ('//*[@id=\"ctl00_CPH_GridViewEpisodes\"]/tbody', 7),\n",
    "        ('//*[@id=\"graph_eq\"]/table/tbody/tr/td/table/tbody', 0),\n",
    "        ('//*[@id=\"provinces\"]/table/tbody', 3),\n",
    "        ('//*[@id=\"cities\"]/table/tbody', 6),\n",
    "        ('//*[@id=\"airports\"]/table/tbody', 8),\n",
    "        ('//*[@id=\"ports\"]/table/tbody', 4),\n",
    "        ('//*[@id=\"dams\"]/table/tbody', 5),\n",
    "        ('//*[@id=\"nuclear\"]/table/tbody', 4)\n",
    "    ]\n",
    "\n",
    "    # Process each table\n",
    "    tables_data = []\n",
    "    for i, (xpath, col_limit) in enumerate(table_configs):\n",
    "        table_data = extract_table_data(driver, event_id, episode, xpath, col_limit)\n",
    "        tables_data.append(table_data)\n",
    "    for row in tables_data:\n",
    "        print(row)\n",
    "    driver.quit()\n",
    "\n",
    "    # Define CSV destination\n",
    "    destination_folder = 'D:/Web Scraping/Web-Scraping/CSV/test/'\n",
    "\n",
    "    # Generate CSV files\n",
    "    for i, table_data in enumerate(tables_data):\n",
    "        csv_file_path = f'{destination_folder}{i}.csv'\n",
    "        file_exists = os.path.exists(csv_file_path)\n",
    "        header = eval(f\"table_{i+1}_headers\")\n",
    "        print(i,header)\n",
    "        print(table_data)\n",
    "        df = pd.DataFrame(table_data,columns=header)\n",
    "        df.head()\n",
    "        df.to_csv(csv_file_path, mode='a' if file_exists else 'w', header=not file_exists, index=False)\n",
    "        print(f\"Data {'appended to' if file_exists else 'written to'} {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004452 10 Wildfires\n",
      "[['1004452', '10', 'Republic of Korea', '04 Mar 2022 - 12 Mar 2022', '8', '1580 in the burned area', '19748 ha', 'This forest fire is expected to have a low humanitarian impact based on the magnitude and the affected population and their vulnerability.']]\n",
      "[['1004452', '10', '1', '', '0.5', '1044', '7992', '05 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '2', '', '0.5', '1044', '8053', '05 Mar 2022 11:37', 'GWIS'], ['1004452', '10', '3', '', '0.5', '1539', '12682', '06 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '4', '', '0.5', '1570', '15222', '07 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '5', '', '0.5', '1580', '17363', '08 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '6', '', '0.5', '1570', '18099', '09 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '7', '', '0.5', '1580', '18435', '10 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '8', '', '0.5', '1580', '19387', '11 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '9', '', '0.5', '1633', '19728', '12 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '10', '', '0.5', '1580', '19748', '13 Mar 2022 04:00', 'GWIS']]\n",
      "[['1004452', '10', 'Radius', 'Population'], ['1004452', '10', '10 km', '32000 people'], ['1004452', '10', '5 km', '27000 people'], ['1004452', '10', '2 km', '20000 people'], ['1004452', '10', '1 km', '11000 people'], ['1004452', '10', 'Burned Area', '1600 people']]\n",
      "[['1004452', '10', 'Kyongsang-bukto', 'Republic of Korea', '2.9 million people'], ['1004452', '10', 'Kangwon-do', 'Republic of Korea', '1.6 million people']]\n",
      "[['1004452', '10', 'Yeongweol', 'Kang-Won-Do', 'Republic of Korea', 'City', '26000 people', '74 km'], ['1004452', '10', 'Andong', 'Kyongsangbuk-Do', 'Republic of Korea', 'City', '130000 people', '75 km'], ['1004452', '10', 'Gangneung', 'Kang-Won-Do', 'Republic of Korea', 'City', '-', '85 km'], ['1004452', '10', 'Yecheon', 'Kyongsangbuk-Do', 'Republic of Korea', 'City', '-', '89 km']]\n",
      "[['1004452', '10', 'R 430', '', '2', '', '', '', '0', '39 km'], ['1004452', '10', 'Samchok', 'SUK', 'unknown', '', '', '', '0', '46 km'], ['1004452', '10', 'Gangneung', 'KAG', '11', 'Mil.', 'Paved', 'Yes', '8900', '83 km'], ['1004452', '10', 'R 418', '', '283', '', '', '', '0', '87 km'], ['1004452', '10', 'R 417', '', '525', '', '', '', '0', '92 km']]\n",
      "[['1004452', '10', 'Ulchin', '', 'South Korea', '14 km'], ['1004452', '10', 'Tonghae', 'KRTGH', 'South Korea', '50 km'], ['1004452', '10', 'Mukho', 'KRMUK', 'South Korea', '56 km']]\n",
      "[['1004452', '10', '', 'Imha', 'Panbyon', '1993', '69 km'], ['1004452', '10', '', 'An Dong', 'Nakdong', '1977', '71 km'], ['1004452', '10', '', 'Toam', 'Song', 'n/a', '80 km']]\n",
      "[['1004452', '10', 'ULCHIN', 'REPUBLIC OF KOREA', '6', '8 km']]\n",
      "0 ['Event_id', 'Episode', 'Countries', 'Start_date_last_detected', 'Duration', 'People_affected', 'Burned_area', 'Event_summary']\n",
      "[['1004452', '10', 'Republic of Korea', '04 Mar 2022 - 12 Mar 2022', '8', '1580 in the burned area', '19748 ha', 'This forest fire is expected to have a low humanitarian impact based on the magnitude and the affected population and their vulnerability.']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/0.csv\n",
      "1 ['Event_id', 'Episode', 'ID', 'Alert_Color', 'GDACS_Score', 'Population_Affected', 'Burned_Area', 'Last_Update', 'GWIS']\n",
      "[['1004452', '10', '1', '', '0.5', '1044', '7992', '05 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '2', '', '0.5', '1044', '8053', '05 Mar 2022 11:37', 'GWIS'], ['1004452', '10', '3', '', '0.5', '1539', '12682', '06 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '4', '', '0.5', '1570', '15222', '07 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '5', '', '0.5', '1580', '17363', '08 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '6', '', '0.5', '1570', '18099', '09 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '7', '', '0.5', '1580', '18435', '10 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '8', '', '0.5', '1580', '19387', '11 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '9', '', '0.5', '1633', '19728', '12 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '10', '', '0.5', '1580', '19748', '13 Mar 2022 04:00', 'GWIS']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/1.csv\n",
      "2 ['Event_id', 'Episode', 'Radius', 'Population']\n",
      "[['1004452', '10', 'Radius', 'Population'], ['1004452', '10', '10 km', '32000 people'], ['1004452', '10', '5 km', '27000 people'], ['1004452', '10', '2 km', '20000 people'], ['1004452', '10', '1 km', '11000 people'], ['1004452', '10', 'Burned Area', '1600 people']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/2.csv\n",
      "3 ['Event_id', 'Episode', 'Region_province', 'Country', 'Population']\n",
      "[['1004452', '10', 'Kyongsang-bukto', 'Republic of Korea', '2.9 million people'], ['1004452', '10', 'Kangwon-do', 'Republic of Korea', '1.6 million people']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/3.csv\n",
      "4 ['Event_id', 'Episode', 'Name', 'Region_Province', 'Country', 'City_class', 'Population', 'Distance']\n",
      "[['1004452', '10', 'Yeongweol', 'Kang-Won-Do', 'Republic of Korea', 'City', '26000 people', '74 km'], ['1004452', '10', 'Andong', 'Kyongsangbuk-Do', 'Republic of Korea', 'City', '130000 people', '75 km'], ['1004452', '10', 'Gangneung', 'Kang-Won-Do', 'Republic of Korea', 'City', '-', '85 km'], ['1004452', '10', 'Yecheon', 'Kyongsangbuk-Do', 'Republic of Korea', 'City', '-', '89 km']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/4.csv\n",
      "5 ['Event_id', 'Episode', 'Name', 'IATA_Code', 'Elevation_in_m', 'Usage', 'Runway_type', 'IFR', 'Runway_Length_in_ft', 'Distance']\n",
      "[['1004452', '10', 'R 430', '', '2', '', '', '', '0', '39 km'], ['1004452', '10', 'Samchok', 'SUK', 'unknown', '', '', '', '0', '46 km'], ['1004452', '10', 'Gangneung', 'KAG', '11', 'Mil.', 'Paved', 'Yes', '8900', '83 km'], ['1004452', '10', 'R 418', '', '283', '', '', '', '0', '87 km'], ['1004452', '10', 'R 417', '', '525', '', '', '', '0', '92 km']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/5.csv\n",
      "6 ['Event_id', 'Episode', 'Name', 'LOCODE', 'Country', 'Distance']\n",
      "[['1004452', '10', 'Ulchin', '', 'South Korea', '14 km'], ['1004452', '10', 'Tonghae', 'KRTGH', 'South Korea', '50 km'], ['1004452', '10', 'Mukho', 'KRMUK', 'South Korea', '56 km']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/6.csv\n",
      "7 ['Event_id', 'Episode', 'Reservoir', 'Dam_Name', 'River', 'Year', 'Distance']\n",
      "[['1004452', '10', '', 'Imha', 'Panbyon', '1993', '69 km'], ['1004452', '10', '', 'An Dong', 'Nakdong', '1977', '71 km'], ['1004452', '10', '', 'Toam', 'Song', 'n/a', '80 km']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/7.csv\n",
      "8 ['Event_id', 'Episode', 'Name', 'Country', 'Reactor', 'Distance']\n",
      "[['1004452', '10', 'ULCHIN', 'REPUBLIC OF KOREA', '6', '8 km']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/8.csv\n"
     ]
    }
   ],
   "source": [
    "def list_filenames(folder_path):\n",
    "    try:\n",
    "        # Get a list of all files in the specified folder\n",
    "        filenames = os.listdir(folder_path)\n",
    "        \n",
    "        return filenames\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"F:/Web Scraping/latest_htmls/Wildfires/\"  # Replace with the actual folder path\n",
    "filenames = list_filenames(folder_path)\n",
    "\n",
    "for file in filenames:\n",
    "    wildfire_csv(f\"file:///F:/Web Scraping/latest_htmls/Wildfires/{file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
