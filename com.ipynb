{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"â€“disable-extensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Wildfire Column headers\n",
    "# table_0_headers = ['Event_id','Episode','Event_type','Impact_url']\n",
    "# table_1_headers = ['Event_id','Episode','Countries','Start_date_last_detected','Duration','People_affected','Burned_area','Event_summary']\n",
    "# table_2_headers = ['Event_id','Episode','ID','Alert_Color','GDACS_Score','Population_Affected','Burned_Area','Last_Update','GWIS']\n",
    "# table_3_headers = ['Event_id','Episode','Radius','Population']\n",
    "# table_4_headers = ['Event_id','Episode','Region_province','Country','Population']\n",
    "# table_5_headers = ['Event_id','Episode','Name','Region_Province','Country','City_class','Population','Distance']\n",
    "# table_6_headers = ['Event_id','Episode','Name','IATA_Code','Elevation_in_m','Usage','Runway_type','IFR','Runway_Length_in_ft','Distance']\n",
    "# table_7_headers = ['Event_id','Episode','Name','LOCODE','Country','Distance']\n",
    "# table_8_headers = ['Event_id','Episode','Reservoir','Dam_Name','River','Year','Distance']\n",
    "# table_9_headers = ['Event_id','Episode','Name','Country','Reactor','Distance']\n",
    "# # Function to extract table data based on XPath and the number of columns required\n",
    "# def extract_table_data(driver, event_id, episode, xpath, col_limit):\n",
    "#     table_data = []\n",
    "#     try:\n",
    "#         table = driver.find_element(By.XPATH, xpath)\n",
    "#         rows = table.find_elements(By.XPATH, './/tr')\n",
    "#         for row in rows:\n",
    "#             columns = row.find_elements(By.XPATH, './/td' if col_limit else './/th | .//td')\n",
    "#             row_data = [col.text.strip() for col in columns]\n",
    "#             if len(row_data) >= 1:\n",
    "#                 table_data.append([event_id, episode] + row_data[:col_limit])\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with XPath '{xpath}' not found.\")\n",
    "#     return table_data\n",
    "\n",
    "# # Main function\n",
    "# def wildfire_csv(html_file):\n",
    "#     # File and event details\n",
    "#     file_name = html_file.split('/')[-1].split('.')[0]\n",
    "#     event_id, episode, event_type = file_name.split('_')[0:3]\n",
    "#     print(event_id, episode, event_type)\n",
    "\n",
    "#     # Initialize the WebDriver\n",
    "#     driver = webdriver.Chrome(options=chrome_options)\n",
    "#     driver.get(html_file)\n",
    "\n",
    "#     # Extract event summary text\n",
    "#     event_summary_text = \"\"\n",
    "#     try:\n",
    "#         event_summary_text = driver.find_element(By.XPATH, '//*[@class=\"p_summary\"][1]').text\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Event summary not found.\")\n",
    "\n",
    "#     # Define table configurations (XPaths and column limits)\n",
    "#     table_configs = [\n",
    "#         ('//*[@id=\"alert_summary_left\"]/table/tbody', 6),\n",
    "#         ('//*[@id=\"ctl00_CPH_GridViewEpisodes\"]/tbody', 7),\n",
    "#         ('//*[@id=\"graph_eq\"]/table/tbody/tr/td/table/tbody', 2),\n",
    "#         ('//*[@id=\"provinces\"]/table/tbody', 3),\n",
    "#         ('//*[@id=\"cities\"]/table/tbody', 6),\n",
    "#         ('//*[@id=\"airports\"]/table/tbody', 8),\n",
    "#         ('//*[@id=\"ports\"]/table/tbody', 4),\n",
    "#         ('//*[@id=\"dams\"]/table/tbody', 5),\n",
    "#         ('//*[@id=\"nuclear\"]/table/tbody', 4)\n",
    "#     ]\n",
    "\n",
    "#     # Process each table\n",
    "#     tables_data = []\n",
    "#     for i, (xpath, col_limit) in enumerate(table_configs):\n",
    "#         table_data = extract_table_data(driver, event_id, episode, xpath, col_limit)\n",
    "#         if i == 0:  # Add event summary for the first table\n",
    "#             for row in table_data:\n",
    "#                 row.append(event_summary_text)\n",
    "#         tables_data.append(table_data)\n",
    "\n",
    "#     driver.quit()\n",
    "\n",
    "#     # Define CSV destination\n",
    "#     destination_folder = 'D:/Web Scraping/Web-Scraping/CSV/test/'\n",
    "\n",
    "#     # Generate CSV files\n",
    "#     for i, table_data in enumerate(tables_data, start=1):\n",
    "#         csv_file_path = f'{destination_folder}{i}.csv'\n",
    "#         file_exists = os.path.exists(csv_file_path)\n",
    "#         header = eval(f\"table_{i}_headers\")\n",
    "#         df = pd.DataFrame(table_data,columns=header)\n",
    "#         df.to_csv(csv_file_path, mode='a' if file_exists else 'w', header=not file_exists, index=False)\n",
    "#         print(f\"Data {'appended to' if file_exists else 'written to'} {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wildfire_csv(html_file):\n",
    "#     file_name = html_file.split('/')[-1].split('.')[0]\n",
    "#     event_id,episode,event_type = file_name.split('_')[0:3]\n",
    "\n",
    "#     print(event_id,episode,event_type)\n",
    "\n",
    "\n",
    "\n",
    "#     # Initialize the WebDriver\n",
    "#     driver = webdriver.Chrome(options=chrome_options)\n",
    "#     driver.get(html_file)\n",
    "        \n",
    "#     try:\n",
    "#         event_path = '//*[@class=\"p_summary\"][1]'\n",
    "#         event_summary_element = driver.find_element(By.XPATH, event_path)\n",
    "#         event_summary_text = event_summary_element.text\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with the provided XPath '{event_path}' not found.\")\n",
    "\n",
    "#     # table_0_df =[event_id,episode,event_type]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     table_1_df =[]\n",
    "#     try:\n",
    "#         table_xpath = '//*[@id=\"alert_summary_left\"]/table/tbody'  # Adjust XPath as needed\n",
    "#         table = driver.find_element(By.XPATH, table_xpath)\n",
    "#         # Fetch table rows using XPath\n",
    "#         rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "#         rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "#         # Extract data from each row and column\n",
    "#         table_data = []\n",
    "#         for row in rows:\n",
    "#             columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "#             columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "#             row_data = [column.text for column in columns]\n",
    "#             if len(row_data) <1:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 table_data.append(row_data)\n",
    "\n",
    "\n",
    "#         row = [\n",
    "#             event_id,\n",
    "#             episode,\n",
    "#             *[table_data[i][1] for i in range(1, 6)],\n",
    "#             event_summary_text\n",
    "#         ]\n",
    "#         table_1_df.append(row)\n",
    "#         # for row in table_1_df:\n",
    "#         #     print(row)\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n",
    "\n",
    "\n",
    "\n",
    "#     # table 2\n",
    "#     table_2_df = []\n",
    "#     try:\n",
    "#         # Locate the Impact Timeline\n",
    "#         table_xpath = '//*[@id=\"ctl00_CPH_GridViewEpisodes\"]/tbody'  # Adjust XPath as needed\n",
    "#         table = driver.find_element(By.XPATH, table_xpath)\n",
    "#         # Fetch table rows using XPath\n",
    "#         rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "#         rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "#         # Extract data from each row and column\n",
    "#         table_data = []\n",
    "#         for row in rows:\n",
    "#             columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "#             columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "#             row_data = [column.text for column in columns]\n",
    "#             if len(row_data) <1:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 table_data.append(row_data)\n",
    "#         for i in range(len(table_data)):\n",
    "#             row = [\n",
    "#                 event_id,\n",
    "#                 episode,\n",
    "#                 *table_data[i][:7] \n",
    "#             ]\n",
    "#             table_2_df.append(row)\n",
    "\n",
    "#         # for row in table_2_df:\n",
    "#         #     print(row)\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     table_3_df = []\n",
    "#     try:\n",
    "#         # Locate the Exposed population\n",
    "#         table_xpath = '//*[@id=\"graph_eq\"]/table/tbody/tr/td/table/tbody'  # Adjust XPath as needed\n",
    "#         table = driver.find_element(By.XPATH, table_xpath)\n",
    "#         # Fetch table rows using XPath\n",
    "#         rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "#         rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "#         # Extract data from each row and column\n",
    "#         table_data = []\n",
    "#         for row in rows:\n",
    "#             columns_xpath = './/th | .//td'  # XPath to locate columns within each row\n",
    "#             columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "#             row_data = [column.text for column in columns]\n",
    "#             if len(row_data) <1:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 table_data.append(row_data)\n",
    "\n",
    "#         for i in range(len(table_data)):\n",
    "#             row = [\n",
    "#                 event_id,\n",
    "#                 episode,\n",
    "#                 *table_data[i][:2] \n",
    "#             ]\n",
    "#             table_3_df.append(row)\n",
    "\n",
    "#         # for row in table_3_df:\n",
    "#         #     print(row)\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n",
    "\n",
    "\n",
    "\n",
    "#     # Table 4\n",
    "#     table_4_df = []\n",
    "#     try:\n",
    "#         # Locate the Affected Provinces\n",
    "#         table_xpath = '//*[@id=\"provinces\"]/table/tbody'  # Adjust XPath as needed\n",
    "#         table = driver.find_element(By.XPATH, table_xpath)\n",
    "#         # Fetch table rows using XPath\n",
    "#         rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "#         rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "#         # Extract data from each row and column\n",
    "#         table_data = []\n",
    "#         for row in rows:\n",
    "#             columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "#             columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "#             row_data = [column.text for column in columns]\n",
    "#             if len(row_data) <1:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 table_data.append(row_data)\n",
    "\n",
    "            \n",
    "#         for i in range(len(table_data)):\n",
    "#             row = [\n",
    "#                 event_id,\n",
    "#                 episode,\n",
    "#                 *table_data[i][:3] \n",
    "#             ]\n",
    "#             table_4_df.append(row)\n",
    "\n",
    "#         # for row in table_4_df:\n",
    "#         #     print(row)\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n",
    "\n",
    "#     table_5_df = []\n",
    "#     try:\n",
    "#         # Locate the Affected populated places\n",
    "#         table_xpath = '//*[@id=\"cities\"]/table/tbody'  # Adjust XPath as needed\n",
    "#         table = driver.find_element(By.XPATH, table_xpath)\n",
    "#         # Fetch table rows using XPath\n",
    "#         rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "#         rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "#         # Extract data from each row and column\n",
    "#         table_data = []\n",
    "#         for row in rows:\n",
    "#             columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "#             columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "#             row_data = [column.text.strip() for column in columns]\n",
    "#             # row_data = [column.text for column in columns]\n",
    "#             if len(row_data) <1:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 table_data.append(row_data)\n",
    "            \n",
    "            \n",
    "#         for i in range(len(table_data)):\n",
    "#             row = [\n",
    "#                 event_id,\n",
    "#                 episode,\n",
    "#                 *table_data[i][:6] \n",
    "#             ]\n",
    "#             table_5_df.append(row)\n",
    "\n",
    "#         # for row in table_5_df:\n",
    "#         #     print(row)\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n",
    "\n",
    "\n",
    "\n",
    "#     table_6_df = []\n",
    "#     try:\n",
    "#         # Table 6\n",
    "#         # Locate the airports\n",
    "#         table_xpath = '//*[@id=\"airports\"]/table/tbody'  # Adjust XPath as needed\n",
    "#         table = driver.find_element(By.XPATH, table_xpath)\n",
    "#         # Fetch table rows using XPath\n",
    "#         rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "#         rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "#         # Extract data from each row and column\n",
    "#         table_data = []\n",
    "#         for row in rows:\n",
    "#             columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "#             columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "#             row_data = [column.text.strip() for column in columns]\n",
    "#             # row_data = [column.text for column in columns]\n",
    "#             if len(row_data) <1:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 table_data.append(row_data)\n",
    "            \n",
    "            \n",
    "#         for i in range(len(table_data)):\n",
    "#             row = [\n",
    "#                 event_id,\n",
    "#                 episode,\n",
    "#                 *table_data[i][:8] \n",
    "#             ]\n",
    "#             table_6_df.append(row)\n",
    "\n",
    "#         # for row in table_6_df:\n",
    "#         #     print(row)\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Table 7\n",
    "#     # Locate the ports\n",
    "#     table_7_df = []\n",
    "#     try:\n",
    "#         table_xpath = '//*[@id=\"ports\"]/table/tbody'  # Adjust XPath as needed\n",
    "#         table = driver.find_element(By.XPATH, table_xpath)\n",
    "#         # Fetch table rows using XPath\n",
    "#         rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "#         rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "#         # Extract data from each row and column\n",
    "#         table_data = []\n",
    "#         for row in rows:\n",
    "#             columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "#             columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "#             row_data = [column.text.strip() for column in columns]\n",
    "#             # row_data = [column.text for column in columns]\n",
    "#             if len(row_data) <1:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 table_data.append(row_data)\n",
    "            \n",
    "            \n",
    "#         for i in range(len(table_data)):\n",
    "#             row = [\n",
    "#                 event_id,\n",
    "#                 episode,\n",
    "#                 *table_data[i][:4] \n",
    "#             ]\n",
    "#             table_7_df.append(row)\n",
    "\n",
    "#         # for row in table_7_df:\n",
    "#         #     print(row)\n",
    "            \n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n",
    "        \n",
    "\n",
    "#     table_8_df = []\n",
    "#     # Table 8\n",
    "#     # Locate the dams\n",
    "#     try:\n",
    "#         table_xpath = '//*[@id=\"dams\"]/table/tbody'  # Adjust XPath as needed\n",
    "#         table = driver.find_element(By.XPATH, table_xpath)\n",
    "#         # Fetch table rows using XPath\n",
    "#         rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "#         rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "#         # Extract data from each row and column\n",
    "#         table_data = []\n",
    "#         for row in rows:\n",
    "#             columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "#             columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "#             row_data = [column.text.strip() for column in columns]\n",
    "#             # row_data = [column.text for column in columns]\n",
    "#             if len(row_data) <1:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 table_data.append(row_data)\n",
    "            \n",
    "#         for i in range(len(table_data)):\n",
    "#             row = [\n",
    "#                 event_id,\n",
    "#                 episode,\n",
    "#                 *table_data[i][:5] \n",
    "#             ]\n",
    "#             table_8_df.append(row)\n",
    "\n",
    "#         # for row in table_8_df:\n",
    "#         #     print(row)\n",
    "            \n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n",
    "        \n",
    "        \n",
    "#     table_9_df = []\n",
    "#     # Table 9\n",
    "#     # Locate the nuclear plants\n",
    "#     try:\n",
    "#         table_xpath = '//*[@id=\"nuclear\"]/table/tbody'  # Adjust XPath as needed\n",
    "#         table = driver.find_element(By.XPATH, table_xpath)\n",
    "#         # Fetch table rows using XPath\n",
    "#         rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "#         rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "#         # Extract data from each row and column\n",
    "#         table_data = []\n",
    "#         for row in rows:\n",
    "#             columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "#             columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "#             row_data = [column.text.strip() for column in columns]\n",
    "#             # row_data = [column.text for column in columns]\n",
    "#             if len(row_data) <1:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 table_data.append(row_data)\n",
    "            \n",
    "#         for i in range(len(table_data)):\n",
    "#             row = [\n",
    "#                 event_id,\n",
    "#                 episode,\n",
    "#                 *table_data[i][:5] \n",
    "#             ]\n",
    "#             table_9_df.append(row)\n",
    "\n",
    "#         # for row in table_9_df:\n",
    "#         #     print(row)\n",
    "            \n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n",
    "\n",
    "#     driver.quit()\n",
    "\n",
    "#     # Generate CSV Files\n",
    "#     destination_folder = 'D:\\Web Scraping\\Web-Scraping\\CSV\\Wildfires\\\\'\n",
    "#     for i in range(1,10):\n",
    "#         header = eval(f\"table_{i}_headers\")\n",
    "#         tbl = eval(f'table_{i}_df')\n",
    "#         df = pd.DataFrame(tbl,columns=header)\n",
    "#         # print(df.head())\n",
    "#         csv_file_path = destination_folder+f'{i}.csv'\n",
    "#         # Check if the file exists\n",
    "#         file_exists = os.path.exists(csv_file_path)\n",
    "#             # Append to the file\n",
    "        \n",
    "#         # Write or append to the CSV file\n",
    "#         # df.to_csv(csv_file_path, mode= 'w', header= True, index=False)\n",
    "#         df.to_csv(csv_file_path, mode='a' if file_exists else 'w', header=not file_exists, index=False)\n",
    "#         print(f\"Data {'appended to' if file_exists else 'written to'} {csv_file_path}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Wildfire Column headers\n",
    "# table_headers = {\n",
    "#     # 0: ['Event_id', 'Episode', 'Event_type', 'Impact_url'],\n",
    "#     1: ['Event_id', 'Episode', 'Countries', 'Start_date_last_detected', 'Duration', 'People_affected', 'Burned_area', 'Event_summary'],\n",
    "#     2: ['Event_id', 'Episode', 'ID', 'Alert_Color', 'GDACS_Score', 'Population_Affected', 'Burned_Area', 'Last_Update', 'GWIS'],\n",
    "#     3: ['Event_id', 'Episode', 'Radius', 'Population'],\n",
    "#     4: ['Event_id', 'Episode', 'Region_province', 'Country', 'Population'],\n",
    "#     5: ['Event_id', 'Episode', 'Name', 'Region_Province', 'Country', 'City_class', 'Population', 'Distance'],\n",
    "#     6: ['Event_id', 'Episode', 'Name', 'IATA_Code', 'Elevation_in_m', 'Usage', 'Runway_type', 'IFR', 'Runway_Length_in_ft', 'Distance'],\n",
    "#     7: ['Event_id', 'Episode', 'Name', 'LOCODE', 'Country', 'Distance'],\n",
    "#     8: ['Event_id', 'Episode', 'Reservoir', 'Dam_Name', 'River', 'Year', 'Distance'],\n",
    "#     9: ['Event_id', 'Episode', 'Name', 'Country', 'Reactor', 'Distance']\n",
    "# }\n",
    "\n",
    "# # Configuration dictionary for table rules\n",
    "# table_configs = {\n",
    "#     # 0: {'xpath': '//*[@id=\"alert_summary_left\"]/table/tbody', 'col_limit': 6, 'append_summary': True},\n",
    "#     1: {'xpath': '//*[@id=\"ctl00_CPH_GridViewEpisodes\"]/tbody', 'col_limit': 7},\n",
    "#     2: {'xpath': '//*[@id=\"graph_eq\"]/table/tbody/tr/td/table/tbody', 'col_limit': None},\n",
    "#     3: {'xpath': '//*[@id=\"provinces\"]/table/tbody', 'col_limit': 3},\n",
    "#     4: {'xpath': '//*[@id=\"cities\"]/table/tbody', 'col_limit': 6},\n",
    "#     5: {'xpath': '//*[@id=\"airports\"]/table/tbody', 'col_limit': 8},\n",
    "#     6: {'xpath': '//*[@id=\"ports\"]/table/tbody', 'col_limit': 4},\n",
    "#     7: {'xpath': '//*[@id=\"dams\"]/table/tbody', 'col_limit': 5},\n",
    "#     8: {'xpath': '//*[@id=\"nuclear\"]/table/tbody', 'col_limit': 4}\n",
    "# }\n",
    "\n",
    "# # Function to extract table data based on XPath and the number of columns required\n",
    "# def extract_table_data(driver, event_id, episode, xpath, col_limit=None):\n",
    "#     table_data = []\n",
    "#     try:\n",
    "#         table = driver.find_element(By.XPATH, xpath)\n",
    "#         rows = table.find_elements(By.XPATH, './/tr')\n",
    "#         for row in rows:\n",
    "#             columns = row.find_elements(By.XPATH, './/td' if col_limit else './/th | .//td')\n",
    "#             row_data = [col.text.strip() for col in columns]\n",
    "#             if len(row_data) >= 1:\n",
    "#                 table_data.append([event_id, episode] + row_data[:col_limit if col_limit else len(row_data)])\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Table with XPath '{xpath}' not found.\")\n",
    "#     return table_data\n",
    "\n",
    "# # Main function\n",
    "# def wildfire_csv(html_file):\n",
    "#     # File and event details\n",
    "#     file_name = html_file.split('/')[-1].split('.')[0]\n",
    "#     event_id, episode, event_type = file_name.split('_')[0:3]\n",
    "#     print(event_id, episode, event_type)\n",
    "\n",
    "#     # Initialize the WebDriver\n",
    "#     driver = webdriver.Chrome(options=chrome_options)\n",
    "#     driver.get(html_file)\n",
    "\n",
    "#     # Extract event summary text\n",
    "#     event_summary_text = \"\"\n",
    "#     try:\n",
    "#         event_summary_text = driver.find_element(By.XPATH, '//*[@class=\"p_summary\"][1]').text\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Event summary not found.\")\n",
    "\n",
    "#     # Process each table\n",
    "#     tables_data = []\n",
    "#     for i, config in table_configs.items():\n",
    "#         table_data = extract_table_data(driver, event_id, episode, config['xpath'], config.get('col_limit'))\n",
    "#         # Append event summary if needed\n",
    "#         if config.get('append_summary'):\n",
    "#             for row in table_data:\n",
    "#                 row.append(event_summary_text)\n",
    "#         tables_data.append(table_data)\n",
    "\n",
    "#     driver.quit()\n",
    "\n",
    "#     # Define CSV destination\n",
    "#     destination_folder = 'D:/Web Scraping/Web-Scraping/CSV/test/'\n",
    "\n",
    "#     # Generate CSV files\n",
    "#     for i, table_data in enumerate(tables_data, start=1):\n",
    "#         csv_file_path = f'{destination_folder}{i}.csv'\n",
    "#         file_exists = os.path.exists(csv_file_path)\n",
    "#         header = table_headers[i]\n",
    "#         print(i, header)\n",
    "#         print(table_data)\n",
    "#         df = pd.DataFrame(table_data, columns=header)\n",
    "#         df.to_csv(csv_file_path, mode='a' if file_exists else 'w', header=not file_exists, index=False)\n",
    "#         print(f\"Data {'appended to' if file_exists else 'written to'} {csv_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Wildfire Column headers\n",
    "# table_0_headers = ['Event_id','Episode','Event_type','Impact_url']\n",
    "# table_1_headers = ['Event_id','Episode','Countries','Start_date_last_detected','Duration','People_affected','Burned_area','Event_summary']\n",
    "# table_2_headers = ['Event_id','Episode','ID','Alert_Color','GDACS_Score','Population_Affected','Burned_Area','Last_Update','GWIS']\n",
    "# table_3_headers = ['Event_id','Episode','Radius','Population']\n",
    "# table_4_headers = ['Event_id','Episode','Region_province','Country','Population']\n",
    "# table_5_headers = ['Event_id','Episode','Name','Region_Province','Country','City_class','Population','Distance']\n",
    "# table_6_headers = ['Event_id','Episode','Name','IATA_Code','Elevation_in_m','Usage','Runway_type','IFR','Runway_Length_in_ft','Distance']\n",
    "# table_7_headers = ['Event_id','Episode','Name','LOCODE','Country','Distance']\n",
    "# table_8_headers = ['Event_id','Episode','Reservoir','Dam_Name','River','Year','Distance']\n",
    "# table_9_headers = ['Event_id','Episode','Name','Country','Reactor','Distance']\n",
    "# # Function to extract table data based on XPath and the number of columns required\n",
    "# def extract_table_data(driver, event_id, episode, xpath, col_limit):\n",
    "#     table_data = []\n",
    "#     if xpath == '//*[@id=\"ctl00_CPH_GridViewEpisodes\"]/tbody':\n",
    "#         print(f\"Table with XPath '{xpath}' not found.\")\n",
    "#         return table_data\n",
    "#     else:\n",
    "#         try:\n",
    "#             table = driver.find_element(By.XPATH, xpath)\n",
    "#             rows = table.find_elements(By.XPATH, './/tr')\n",
    "#             for row in rows:\n",
    "#                 columns = row.find_elements(By.XPATH, './/td' if col_limit else './/th | .//td')\n",
    "#                 row_data = [col.text.strip() for col in columns]\n",
    "#                 if len(row_data) >= 1:\n",
    "#                     table_data.append([event_id, episode] + row_data[:col_limit if col_limit else len(row_data)])\n",
    "#         except NoSuchElementException:\n",
    "#             print(f\"Table with XPath '{xpath}' not found.\")\n",
    "#         return table_data\n",
    "\n",
    "# # Main function\n",
    "# def wildfire_csv(html_file):\n",
    "#     # File and event details\n",
    "#     file_name = html_file.split('/')[-1].split('.')[0]\n",
    "#     event_id, episode, event_type = file_name.split('_')[0:3]\n",
    "#     print(event_id, episode, event_type)\n",
    "\n",
    "#     # Initialize the WebDriver\n",
    "#     driver = webdriver.Chrome(options=chrome_options)\n",
    "#     driver.get(html_file)\n",
    "\n",
    "#     # Extract event summary text\n",
    "#     event_summary_text = \"\"\n",
    "#     try:\n",
    "#         event_summary_text = driver.find_element(By.XPATH, '//*[@class=\"p_summary\"][1]').text\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Event summary not found.\")\n",
    "\n",
    "#     # Define table configurations (XPaths and column limits)\n",
    "#     table_configs = [\n",
    "#         ('//*[@id=\"alert_summary_left\"]/table/tbody', 6),\n",
    "#         ('//*[@id=\"ctl00_CPH_GridViewEpisodes\"]/tbody', 7),\n",
    "#         ('//*[@id=\"graph_eq\"]/table/tbody/tr/td/table/tbody', 0),\n",
    "#         ('//*[@id=\"provinces\"]/table/tbody', 3),\n",
    "#         ('//*[@id=\"cities\"]/table/tbody', 6),\n",
    "#         ('//*[@id=\"airports\"]/table/tbody', 8),\n",
    "#         ('//*[@id=\"ports\"]/table/tbody', 4),\n",
    "#         ('//*[@id=\"dams\"]/table/tbody', 5),\n",
    "#         ('//*[@id=\"nuclear\"]/table/tbody', 4)\n",
    "#     ]\n",
    "\n",
    "#     # Process each table\n",
    "#     tables_data = []\n",
    "#     for i, (xpath, col_limit) in enumerate(table_configs, start=1):\n",
    "#         table_data = extract_table_data(driver, event_id, episode, xpath, col_limit)\n",
    "#         if i == 0:  # Add event summary for the first table\n",
    "#             for row in table_data:\n",
    "#                 row.append(event_summary_text)\n",
    "#         tables_data.append(table_data)\n",
    "\n",
    "#     driver.quit()\n",
    "\n",
    "#     # Define CSV destination\n",
    "#     destination_folder = 'D:/Web Scraping/Web-Scraping/CSV/test/'\n",
    "\n",
    "#     # Generate CSV files\n",
    "#     for i, table_data in enumerate(tables_data):\n",
    "#         csv_file_path = f'{destination_folder}{i}.csv'\n",
    "#         file_exists = os.path.exists(csv_file_path)\n",
    "#         header = eval(f\"table_{i}_headers\")\n",
    "#         print(i,header)\n",
    "#         print(table_data)\n",
    "#         df = pd.DataFrame(table_data,columns=header)\n",
    "#         df.head()\n",
    "#         df.to_csv(csv_file_path, mode='a' if file_exists else 'w', header=not file_exists, index=False)\n",
    "#         print(f\"Data {'appended to' if file_exists else 'written to'} {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wildfire Column headers\n",
    "table_0_headers = ['Event_id','Episode','Event_type','Impact_url']\n",
    "table_1_headers = ['Event_id','Episode','Countries','Start_date_last_detected','Duration','People_affected','Burned_area','Event_summary']\n",
    "table_2_headers = ['Event_id','Episode','ID','Alert_Color','GDACS_Score','Population_Affected','Burned_Area','Last_Update','GWIS']\n",
    "table_3_headers = ['Event_id','Episode','Radius','Population']\n",
    "table_4_headers = ['Event_id','Episode','Region_province','Country','Population']\n",
    "table_5_headers = ['Event_id','Episode','Name','Region_Province','Country','City_class','Population','Distance']\n",
    "table_6_headers = ['Event_id','Episode','Name','IATA_Code','Elevation_in_m','Usage','Runway_type','IFR','Runway_Length_in_ft','Distance']\n",
    "table_7_headers = ['Event_id','Episode','Name','LOCODE','Country','Distance']\n",
    "table_8_headers = ['Event_id','Episode','Reservoir','Dam_Name','River','Year','Distance']\n",
    "table_9_headers = ['Event_id','Episode','Name','Country','Reactor','Distance']\n",
    "# Function to extract table data based on XPath and the number of columns required\n",
    "def extract_table_data(driver, event_id, episode, xpath, col_limit):\n",
    "    table_data = []\n",
    "    if xpath == '//*[@id=\"alert_summary_left\"]/table/tbody':\n",
    "        try:\n",
    "            temp_tbl=[]\n",
    "            table = driver.find_element(By.XPATH, xpath)\n",
    "            rows = table.find_elements(By.XPATH, './/tr')\n",
    "            event_summary_text = driver.find_element(By.XPATH, '//*[@class=\"p_summary\"][1]').text\n",
    "            for row in rows:\n",
    "                columns = row.find_elements(By.XPATH, './/td')\n",
    "                row_data = [column.text for column in columns]\n",
    "                if len(row_data)>=1:\n",
    "                    temp_tbl.append(row_data)\n",
    "            table_data = [[event_id, episode,*[temp_tbl[i][1] for i in range(1,col_limit)]\n",
    "                        ,event_summary_text]]\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Event summary not founds or table with XPath '{xpath}' not found.\")\n",
    "        return table_data\n",
    "    else:\n",
    "        try:\n",
    "            table = driver.find_element(By.XPATH, xpath)\n",
    "            rows = table.find_elements(By.XPATH, './/tr')\n",
    "            for row in rows:\n",
    "                columns = row.find_elements(By.XPATH, './/td' if col_limit else './/th | .//td')\n",
    "                row_data = [col.text.strip() for col in columns]\n",
    "                if len(row_data) >= 1:\n",
    "                    table_data.append([event_id, episode] + row_data[:col_limit if col_limit else len(row_data)])\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Table with XPath '{xpath}' not found.\")\n",
    "        return table_data\n",
    "\n",
    "# Main function\n",
    "def wildfire_csv(html_file):\n",
    "    # File and event details\n",
    "    file_name = html_file.split('/')[-1].split('.')[0]\n",
    "    event_id, episode, event_type = file_name.split('_')[0:3]\n",
    "    print(event_id, episode, event_type)\n",
    "\n",
    "    # Initialize the WebDriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(html_file)\n",
    "\n",
    "\n",
    "    # Define table configurations (XPaths and column limits)\n",
    "    table_configs = [\n",
    "        ('//*[@id=\"alert_summary_left\"]/table/tbody', 6),\n",
    "        ('//*[@id=\"ctl00_CPH_GridViewEpisodes\"]/tbody', 7),\n",
    "        ('//*[@id=\"graph_eq\"]/table/tbody/tr/td/table/tbody', 0),\n",
    "        ('//*[@id=\"provinces\"]/table/tbody', 3),\n",
    "        ('//*[@id=\"cities\"]/table/tbody', 6),\n",
    "        ('//*[@id=\"airports\"]/table/tbody', 8),\n",
    "        ('//*[@id=\"ports\"]/table/tbody', 4),\n",
    "        ('//*[@id=\"dams\"]/table/tbody', 5),\n",
    "        ('//*[@id=\"nuclear\"]/table/tbody', 4)\n",
    "    ]\n",
    "\n",
    "    # Process each table\n",
    "    tables_data = []\n",
    "    for i, (xpath, col_limit) in enumerate(table_configs):\n",
    "        table_data = extract_table_data(driver, event_id, episode, xpath, col_limit)\n",
    "        tables_data.append(table_data)\n",
    "    for row in tables_data:\n",
    "        print(row)\n",
    "    driver.quit()\n",
    "\n",
    "    # Define CSV destination\n",
    "    destination_folder = 'D:/Web Scraping/Web-Scraping/CSV/test/'\n",
    "\n",
    "    # Generate CSV files\n",
    "    for i, table_data in enumerate(tables_data):\n",
    "        csv_file_path = f'{destination_folder}{i}.csv'\n",
    "        file_exists = os.path.exists(csv_file_path)\n",
    "        header = eval(f\"table_{i+1}_headers\")\n",
    "        print(i,header)\n",
    "        print(table_data)\n",
    "        df = pd.DataFrame(table_data,columns=header)\n",
    "        df.head()\n",
    "        df.to_csv(csv_file_path, mode='a' if file_exists else 'w', header=not file_exists, index=False)\n",
    "        print(f\"Data {'appended to' if file_exists else 'written to'} {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004452 10 Wildfires\n",
      "[['1004452', '10', 'Republic of Korea', '04 Mar 2022 - 12 Mar 2022', '8', '1580 in the burned area', '19748 ha', 'This forest fire is expected to have a low humanitarian impact based on the magnitude and the affected population and their vulnerability.']]\n",
      "[['1004452', '10', '1', '', '0.5', '1044', '7992', '05 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '2', '', '0.5', '1044', '8053', '05 Mar 2022 11:37', 'GWIS'], ['1004452', '10', '3', '', '0.5', '1539', '12682', '06 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '4', '', '0.5', '1570', '15222', '07 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '5', '', '0.5', '1580', '17363', '08 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '6', '', '0.5', '1570', '18099', '09 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '7', '', '0.5', '1580', '18435', '10 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '8', '', '0.5', '1580', '19387', '11 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '9', '', '0.5', '1633', '19728', '12 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '10', '', '0.5', '1580', '19748', '13 Mar 2022 04:00', 'GWIS']]\n",
      "[['1004452', '10', 'Radius', 'Population'], ['1004452', '10', '10 km', '32000 people'], ['1004452', '10', '5 km', '27000 people'], ['1004452', '10', '2 km', '20000 people'], ['1004452', '10', '1 km', '11000 people'], ['1004452', '10', 'Burned Area', '1600 people']]\n",
      "[['1004452', '10', 'Kyongsang-bukto', 'Republic of Korea', '2.9 million people'], ['1004452', '10', 'Kangwon-do', 'Republic of Korea', '1.6 million people']]\n",
      "[['1004452', '10', 'Yeongweol', 'Kang-Won-Do', 'Republic of Korea', 'City', '26000 people', '74 km'], ['1004452', '10', 'Andong', 'Kyongsangbuk-Do', 'Republic of Korea', 'City', '130000 people', '75 km'], ['1004452', '10', 'Gangneung', 'Kang-Won-Do', 'Republic of Korea', 'City', '-', '85 km'], ['1004452', '10', 'Yecheon', 'Kyongsangbuk-Do', 'Republic of Korea', 'City', '-', '89 km']]\n",
      "[['1004452', '10', 'R 430', '', '2', '', '', '', '0', '39 km'], ['1004452', '10', 'Samchok', 'SUK', 'unknown', '', '', '', '0', '46 km'], ['1004452', '10', 'Gangneung', 'KAG', '11', 'Mil.', 'Paved', 'Yes', '8900', '83 km'], ['1004452', '10', 'R 418', '', '283', '', '', '', '0', '87 km'], ['1004452', '10', 'R 417', '', '525', '', '', '', '0', '92 km']]\n",
      "[['1004452', '10', 'Ulchin', '', 'South Korea', '14 km'], ['1004452', '10', 'Tonghae', 'KRTGH', 'South Korea', '50 km'], ['1004452', '10', 'Mukho', 'KRMUK', 'South Korea', '56 km']]\n",
      "[['1004452', '10', '', 'Imha', 'Panbyon', '1993', '69 km'], ['1004452', '10', '', 'An Dong', 'Nakdong', '1977', '71 km'], ['1004452', '10', '', 'Toam', 'Song', 'n/a', '80 km']]\n",
      "[['1004452', '10', 'ULCHIN', 'REPUBLIC OF KOREA', '6', '8 km']]\n",
      "0 ['Event_id', 'Episode', 'Countries', 'Start_date_last_detected', 'Duration', 'People_affected', 'Burned_area', 'Event_summary']\n",
      "[['1004452', '10', 'Republic of Korea', '04 Mar 2022 - 12 Mar 2022', '8', '1580 in the burned area', '19748 ha', 'This forest fire is expected to have a low humanitarian impact based on the magnitude and the affected population and their vulnerability.']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/0.csv\n",
      "1 ['Event_id', 'Episode', 'ID', 'Alert_Color', 'GDACS_Score', 'Population_Affected', 'Burned_Area', 'Last_Update', 'GWIS']\n",
      "[['1004452', '10', '1', '', '0.5', '1044', '7992', '05 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '2', '', '0.5', '1044', '8053', '05 Mar 2022 11:37', 'GWIS'], ['1004452', '10', '3', '', '0.5', '1539', '12682', '06 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '4', '', '0.5', '1570', '15222', '07 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '5', '', '0.5', '1580', '17363', '08 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '6', '', '0.5', '1570', '18099', '09 Mar 2022 04:01', 'GWIS'], ['1004452', '10', '7', '', '0.5', '1580', '18435', '10 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '8', '', '0.5', '1580', '19387', '11 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '9', '', '0.5', '1633', '19728', '12 Mar 2022 04:00', 'GWIS'], ['1004452', '10', '10', '', '0.5', '1580', '19748', '13 Mar 2022 04:00', 'GWIS']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/1.csv\n",
      "2 ['Event_id', 'Episode', 'Radius', 'Population']\n",
      "[['1004452', '10', 'Radius', 'Population'], ['1004452', '10', '10 km', '32000 people'], ['1004452', '10', '5 km', '27000 people'], ['1004452', '10', '2 km', '20000 people'], ['1004452', '10', '1 km', '11000 people'], ['1004452', '10', 'Burned Area', '1600 people']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/2.csv\n",
      "3 ['Event_id', 'Episode', 'Region_province', 'Country', 'Population']\n",
      "[['1004452', '10', 'Kyongsang-bukto', 'Republic of Korea', '2.9 million people'], ['1004452', '10', 'Kangwon-do', 'Republic of Korea', '1.6 million people']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/3.csv\n",
      "4 ['Event_id', 'Episode', 'Name', 'Region_Province', 'Country', 'City_class', 'Population', 'Distance']\n",
      "[['1004452', '10', 'Yeongweol', 'Kang-Won-Do', 'Republic of Korea', 'City', '26000 people', '74 km'], ['1004452', '10', 'Andong', 'Kyongsangbuk-Do', 'Republic of Korea', 'City', '130000 people', '75 km'], ['1004452', '10', 'Gangneung', 'Kang-Won-Do', 'Republic of Korea', 'City', '-', '85 km'], ['1004452', '10', 'Yecheon', 'Kyongsangbuk-Do', 'Republic of Korea', 'City', '-', '89 km']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/4.csv\n",
      "5 ['Event_id', 'Episode', 'Name', 'IATA_Code', 'Elevation_in_m', 'Usage', 'Runway_type', 'IFR', 'Runway_Length_in_ft', 'Distance']\n",
      "[['1004452', '10', 'R 430', '', '2', '', '', '', '0', '39 km'], ['1004452', '10', 'Samchok', 'SUK', 'unknown', '', '', '', '0', '46 km'], ['1004452', '10', 'Gangneung', 'KAG', '11', 'Mil.', 'Paved', 'Yes', '8900', '83 km'], ['1004452', '10', 'R 418', '', '283', '', '', '', '0', '87 km'], ['1004452', '10', 'R 417', '', '525', '', '', '', '0', '92 km']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/5.csv\n",
      "6 ['Event_id', 'Episode', 'Name', 'LOCODE', 'Country', 'Distance']\n",
      "[['1004452', '10', 'Ulchin', '', 'South Korea', '14 km'], ['1004452', '10', 'Tonghae', 'KRTGH', 'South Korea', '50 km'], ['1004452', '10', 'Mukho', 'KRMUK', 'South Korea', '56 km']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/6.csv\n",
      "7 ['Event_id', 'Episode', 'Reservoir', 'Dam_Name', 'River', 'Year', 'Distance']\n",
      "[['1004452', '10', '', 'Imha', 'Panbyon', '1993', '69 km'], ['1004452', '10', '', 'An Dong', 'Nakdong', '1977', '71 km'], ['1004452', '10', '', 'Toam', 'Song', 'n/a', '80 km']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/7.csv\n",
      "8 ['Event_id', 'Episode', 'Name', 'Country', 'Reactor', 'Distance']\n",
      "[['1004452', '10', 'ULCHIN', 'REPUBLIC OF KOREA', '6', '8 km']]\n",
      "Data written to D:/Web Scraping/Web-Scraping/CSV/test/8.csv\n"
     ]
    }
   ],
   "source": [
    "def list_filenames(folder_path):\n",
    "    try:\n",
    "        # Get a list of all files in the specified folder\n",
    "        filenames = os.listdir(folder_path)\n",
    "        \n",
    "        return filenames\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"F:/Web Scraping/latest_htmls/Wildfires/\"  # Replace with the actual folder path\n",
    "filenames = list_filenames(folder_path)\n",
    "\n",
    "for file in filenames:\n",
    "    wildfire_csv(f\"file:///F:/Web Scraping/latest_htmls/Wildfires/{file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
