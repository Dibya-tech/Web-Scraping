{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def ddlgenerator_mssql(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    table_name = csv_file.split('.')[0]  # Use the filename as the table name without extension\n",
    "    ddl_statements = []\n",
    "    \n",
    "    ddl_statements.append(f\"CREATE TABLE {table_name} (\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = 'VARCHAR(4000)'  # Default type, you can modify the type detection logic as needed\n",
    "        ddl_statements.append(f\"    {col} {col_type},\")\n",
    "    \n",
    "    # Remove the last comma and close the statement\n",
    "    ddl_statements[-1] = ddl_statements[-1].rstrip(',')\n",
    "    ddl_statements.append(\");\")\n",
    "    \n",
    "    ddl_sql = \"\\n\".join(ddl_statements)\n",
    "    \n",
    "    # Save the generated SQL to a .sql file\n",
    "    with open(f\"{table_name}.sql\", 'w') as sql_file:\n",
    "        sql_file.write(ddl_sql)\n",
    "\n",
    "def list_filenames(folder_path):\n",
    "    try:\n",
    "        # Get a list of all files in the specified folder\n",
    "        filenames = os.listdir(folder_path)\n",
    "        return filenames\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def main():\n",
    "    folder_path = \"generated_csv - Copy\"\n",
    "    absolute_folder_path = Path(folder_path).resolve()\n",
    "    filenames = list_filenames(absolute_folder_path)\n",
    "\n",
    "    # Generate SQL DDL for each CSV file\n",
    "    for csv_file in filenames:\n",
    "        absolute_file_path = absolute_folder_path / csv_file\n",
    "        ddlgenerator_mssql(absolute_file_path.as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "(1, )\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Function to read and execute each .sql file\n",
    "def execute_sql_files(folder_path):\n",
    "    # List all .sql files in the folder\n",
    "    sql_files = [f for f in os.listdir(folder_path) if f.endswith('.sql')]\n",
    "\n",
    "    # Loop through each .sql file and execute the commands\n",
    "    for sql_file in sql_files:\n",
    "        file_path = os.path.join(folder_path, sql_file)\n",
    "        print(f\"Executing {sql_file}...\")\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read the entire SQL content from the file\n",
    "            sql_script = file.read()\n",
    "\n",
    "            # Execute the SQL commands\n",
    "            try:\n",
    "                cursor.execute(sql_script)\n",
    "                conn.commit()  # Commit the transaction\n",
    "                print(f\"{sql_file} executed successfully!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing {sql_file}: {e}\")\n",
    "\n",
    "connection_string = (\n",
    "    \"Driver={SQL Server};\"\n",
    "    \"Server=localhost\\MSSQLSERVER01;\"\n",
    "    \"Database=scraping;\"\n",
    "    \"Trusted_Connection=yes;\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Establish the connection\n",
    "    conn = pyodbc.connect(connection_string)\n",
    "    print(\"Connection successful!\")\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Test query: Check the top 1 record from a table, e.g., 'YourTableName'\n",
    "    cursor.execute(\"SELECT 1\")\n",
    "    \n",
    "    # Fetch and display the result\n",
    "    row = cursor.fetchone()\n",
    "    if row:\n",
    "        print(row)\n",
    "    else:\n",
    "        print(\"No data found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing cyclone.affected_areas.sql...\n",
      "cyclone.affected_areas.sql executed successfully!\n",
      "Executing cyclone.airports.sql...\n",
      "cyclone.airports.sql executed successfully!\n",
      "Executing cyclone.cities.sql...\n",
      "cyclone.cities.sql executed successfully!\n",
      "Executing cyclone.countries.sql...\n",
      "cyclone.countries.sql executed successfully!\n",
      "Executing cyclone.dams.sql...\n",
      "cyclone.dams.sql executed successfully!\n",
      "Executing cyclone.impact.sql...\n",
      "cyclone.impact.sql executed successfully!\n",
      "Executing cyclone.nuclear_plants.sql...\n",
      "cyclone.nuclear_plants.sql executed successfully!\n",
      "Executing cyclone.provinces.sql...\n",
      "cyclone.provinces.sql executed successfully!\n",
      "Executing cyclone.summary.sql...\n",
      "cyclone.summary.sql executed successfully!\n",
      "Executing cyclone.timeline.sql...\n",
      "cyclone.timeline.sql executed successfully!\n",
      "Executing cyclone.[ports].sql...\n",
      "cyclone.[ports].sql executed successfully!\n",
      "Executing drought.affected_areas.sql...\n",
      "drought.affected_areas.sql executed successfully!\n",
      "Executing drought.summary.sql...\n",
      "drought.summary.sql executed successfully!\n",
      "Executing drought.timeline.sql...\n",
      "drought.timeline.sql executed successfully!\n",
      "Executing earthquake.nuclear_plants.sql...\n",
      "earthquake.nuclear_plants.sql executed successfully!\n",
      "Executing Earthquakes_Table_1.sql...\n",
      "Earthquakes_Table_1.sql executed successfully!\n",
      "Executing Earthquakes_Table_2.sql...\n",
      "Earthquakes_Table_2.sql executed successfully!\n",
      "Executing Earthquakes_Table_3.sql...\n",
      "Earthquakes_Table_3.sql executed successfully!\n",
      "Executing Earthquakes_Table_4.sql...\n",
      "Earthquakes_Table_4.sql executed successfully!\n",
      "Executing Earthquakes_Table_5.sql...\n",
      "Earthquakes_Table_5.sql executed successfully!\n",
      "Executing Earthquakes_Table_6.sql...\n",
      "Earthquakes_Table_6.sql executed successfully!\n",
      "Executing Earthquakes_Table_7.sql...\n",
      "Earthquakes_Table_7.sql executed successfully!\n",
      "Executing Earthquakes_Table_8.sql...\n",
      "Earthquakes_Table_8.sql executed successfully!\n",
      "Executing Floods_Table_1.sql...\n",
      "Floods_Table_1.sql executed successfully!\n",
      "Executing Floods_Table_2.sql...\n",
      "Floods_Table_2.sql executed successfully!\n",
      "Executing Floods_Table_3.sql...\n",
      "Floods_Table_3.sql executed successfully!\n",
      "Executing Floods_Table_4.sql...\n",
      "Floods_Table_4.sql executed successfully!\n",
      "Executing Floods_Table_5.sql...\n",
      "Floods_Table_5.sql executed successfully!\n",
      "Executing main.sql...\n",
      "Error executing main.sql: ('42S01', \"[42S01] [Microsoft][ODBC SQL Server Driver][SQL Server]There is already an object named 'dimMaster' in the database. (2714) (SQLExecDirectW)\")\n",
      "Executing Volcanoes_Table_1.sql...\n",
      "Volcanoes_Table_1.sql executed successfully!\n",
      "Executing Volcanoes_Table_10.sql...\n",
      "Volcanoes_Table_10.sql executed successfully!\n",
      "Executing Volcanoes_Table_2.sql...\n",
      "Volcanoes_Table_2.sql executed successfully!\n",
      "Executing Volcanoes_Table_3.sql...\n",
      "Volcanoes_Table_3.sql executed successfully!\n",
      "Executing Volcanoes_Table_4.sql...\n",
      "Volcanoes_Table_4.sql executed successfully!\n",
      "Executing Volcanoes_Table_5.sql...\n",
      "Volcanoes_Table_5.sql executed successfully!\n",
      "Executing Volcanoes_Table_6.sql...\n",
      "Volcanoes_Table_6.sql executed successfully!\n",
      "Executing Volcanoes_Table_7.sql...\n",
      "Volcanoes_Table_7.sql executed successfully!\n",
      "Executing Volcanoes_Table_8.sql...\n",
      "Volcanoes_Table_8.sql executed successfully!\n",
      "Executing Volcanoes_Table_9.sql...\n",
      "Volcanoes_Table_9.sql executed successfully!\n",
      "Executing Wildfires_Table_1.sql...\n",
      "Wildfires_Table_1.sql executed successfully!\n",
      "Executing Wildfires_Table_2.sql...\n",
      "Wildfires_Table_2.sql executed successfully!\n",
      "Executing Wildfires_Table_3.sql...\n",
      "Wildfires_Table_3.sql executed successfully!\n",
      "Executing Wildfires_Table_4.sql...\n",
      "Wildfires_Table_4.sql executed successfully!\n",
      "Executing Wildfires_Table_5.sql...\n",
      "Wildfires_Table_5.sql executed successfully!\n",
      "Executing Wildfires_Table_6.sql...\n",
      "Wildfires_Table_6.sql executed successfully!\n",
      "Executing Wildfires_Table_7.sql...\n",
      "Wildfires_Table_7.sql executed successfully!\n",
      "Executing Wildfires_Table_8.sql...\n",
      "Wildfires_Table_8.sql executed successfully!\n",
      "Executing Wildfires_Table_9.sql...\n",
      "Wildfires_Table_9.sql executed successfully!\n"
     ]
    }
   ],
   "source": [
    "# folder_path = \"DDLS\"\n",
    "# execute_sql_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cyclone.affected_areas.sql...\n",
      "Reading cyclone.airports.sql...\n",
      "Reading cyclone.cities.sql...\n",
      "Reading cyclone.countries.sql...\n",
      "Reading cyclone.dams.sql...\n",
      "Reading cyclone.impact.sql...\n",
      "Reading cyclone.nuclear_plants.sql...\n",
      "Reading cyclone.provinces.sql...\n",
      "Reading cyclone.summary.sql...\n",
      "Reading cyclone.timeline.sql...\n",
      "Reading cyclone.[ports].sql...\n",
      "Reading drought.affected_areas.sql...\n",
      "Reading drought.summary.sql...\n",
      "Reading drought.timeline.sql...\n",
      "Reading earthquake.nuclear_plants.sql...\n",
      "Reading Earthquakes_Table_1.sql...\n",
      "Reading Earthquakes_Table_2.sql...\n",
      "Reading Earthquakes_Table_3.sql...\n",
      "Reading Earthquakes_Table_4.sql...\n",
      "Reading Earthquakes_Table_5.sql...\n",
      "Reading Earthquakes_Table_6.sql...\n",
      "Reading Earthquakes_Table_7.sql...\n",
      "Reading Earthquakes_Table_8.sql...\n",
      "Reading Floods_Table_1.sql...\n",
      "Reading Floods_Table_2.sql...\n",
      "Reading Floods_Table_3.sql...\n",
      "Reading Floods_Table_4.sql...\n",
      "Reading Floods_Table_5.sql...\n",
      "Reading main.sql...\n",
      "Reading Volcanoes_Table_1.sql...\n",
      "Reading Volcanoes_Table_10.sql...\n",
      "Reading Volcanoes_Table_2.sql...\n",
      "Reading Volcanoes_Table_3.sql...\n",
      "Reading Volcanoes_Table_4.sql...\n",
      "Reading Volcanoes_Table_5.sql...\n",
      "Reading Volcanoes_Table_6.sql...\n",
      "Reading Volcanoes_Table_7.sql...\n",
      "Reading Volcanoes_Table_8.sql...\n",
      "Reading Volcanoes_Table_9.sql...\n",
      "Reading Wildfires_Table_1.sql...\n",
      "Reading Wildfires_Table_2.sql...\n",
      "Reading Wildfires_Table_3.sql...\n",
      "Reading Wildfires_Table_4.sql...\n",
      "Reading Wildfires_Table_5.sql...\n",
      "Reading Wildfires_Table_6.sql...\n",
      "Reading Wildfires_Table_7.sql...\n",
      "Reading Wildfires_Table_8.sql...\n",
      "Reading Wildfires_Table_9.sql...\n",
      "Merged SQL content written to merged_output.sql\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# # Specify the folder containing your .sql files\n",
    "# sql_folder_path = 'DDLS'\n",
    "# # Specify the output file name\n",
    "# output_file_path = 'merged_output.sql'\n",
    "\n",
    "# # Initialize an empty list to hold the contents of the SQL files\n",
    "# merged_sql_content = []\n",
    "\n",
    "# # Loop through each .sql file in the specified folder\n",
    "# for filename in os.listdir(sql_folder_path):\n",
    "#     if filename.endswith('.sql'):\n",
    "#         file_path = os.path.join(sql_folder_path, filename)\n",
    "#         print(f\"Reading {filename}...\")\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             # Read the file content and append it to the list\n",
    "#             content = file.read()\n",
    "#             merged_sql_content.append(content)\n",
    "\n",
    "# # Write the combined content to the output file\n",
    "# with open(output_file_path, 'w') as output_file:\n",
    "#     for sql_content in merged_sql_content:\n",
    "#         output_file.write(sql_content + '\\n\\n')  # Adding double newline between files for separation\n",
    "\n",
    "# print(f\"Merged SQL content written to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (820948704.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    finally:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# finally:\n",
    "#     # Close the connection\n",
    "#     conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pyodbc\n",
    "import json\n",
    "import math\n",
    "def list_filenames(folder_path):\n",
    "    try:\n",
    "        # Get a list of all files in the specified folder\n",
    "        filenames = os.listdir(folder_path)\n",
    "        return filenames\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "# Load table configs from file\n",
    "def load_table_configs(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "   # Load the table configs \n",
    "filenames_dict = dict(load_table_configs(\"csv_tables.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_into_table(filepath,tablename):\n",
    "    try:\n",
    "        # Establish the connection\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        print(\"Connection successful!\")\n",
    "        data = pd.read_csv(filepath)\n",
    "        df = pd.DataFrame(data)\n",
    "        cursor = conn.cursor()\n",
    "        columns = [f\"[{col}]\" for col in df.columns.tolist()]\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "        \n",
    "        # Dynamically create the SQL query\n",
    "        placeholders = ', '.join(['?' for _ in columns])  # Creates a string like '?, ?, ?'\n",
    "        columns_str = ', '.join(columns)  # Creates a string like 'product_id, product_name, price'\n",
    "        \n",
    "        query = f\"INSERT INTO {tablename} ({columns_str}) VALUES ({placeholders})\"\n",
    "        \n",
    "        for row in df.itertuples(index=False, name=None):\n",
    "            row = tuple(None if isinstance(value, float) and math.isnan(value) else value for value in row)\n",
    "            # print(query, row)\n",
    "            cursor.execute(query, row)\n",
    "        print(f\"Inserted data into {tablename}\")\n",
    "        conn.commit()\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "Inserted data into [cyclone].[summary]\n",
      "Connection successful!\n",
      "Inserted data into [cyclone].[nuclear_plants]\n",
      "Connection successful!\n",
      "Inserted data into [cyclone].[affected_areas]\n",
      "Connection successful!\n",
      "Inserted data into [cyclone].[impact]\n",
      "Connection successful!\n",
      "Inserted data into [cyclone].[timeline]\n",
      "Connection successful!\n",
      "Inserted data into [cyclone].[countries]\n",
      "Connection successful!\n",
      "Inserted data into [cyclone].[provinces]\n",
      "Connection successful!\n",
      "Inserted data into [cyclone].[cities]\n",
      "Connection successful!\n",
      "Inserted data into [cyclone].[airports]\n",
      "Connection successful!\n",
      "Inserted data into [cyclone].[ports]\n",
      "Connection successful!\n",
      "Inserted data into [cyclone].[dams]\n",
      "Connection successful!\n",
      "Inserted data into [drought].[summary]\n",
      "Connection successful!\n",
      "Inserted data into [drought].[timeline]\n",
      "Connection successful!\n",
      "Inserted data into [drought].[countries]\n",
      "Connection successful!\n",
      "Inserted data into [drought].[affected_areas]\n",
      "Connection successful!\n",
      "Inserted data into [earthquake].[summary]\n",
      "Connection successful!\n",
      "Inserted data into [earthquake].[timeline]\n",
      "Connection successful!\n",
      "Inserted data into [earthquake].[impact]\n",
      "Connection successful!\n",
      "Inserted data into [earthquake].[provinces]\n",
      "Connection successful!\n",
      "Inserted data into [earthquake].[cities]\n",
      "Connection successful!\n",
      "Inserted data into [earthquake].[airports]\n",
      "Connection successful!\n",
      "Inserted data into [earthquake].[ports]\n",
      "Connection successful!\n",
      "Inserted data into [earthquake].[dams]\n",
      "Connection successful!\n",
      "Inserted data into [earthquake].[nuclear_plants]\n",
      "Connection successful!\n",
      "Inserted data into [flood].[summary]\n",
      "Connection successful!\n",
      "Inserted data into [flood].[indicator_A]\n",
      "Connection successful!\n",
      "Inserted data into [flood].[indicator_B]\n",
      "Connection successful!\n",
      "Inserted data into [flood].[indicator_C]\n",
      "Connection successful!\n",
      "Inserted data into [flood].[timeline]\n",
      "Connection successful!\n",
      "Inserted data into [dbo].[dimMaster]\n",
      "Connection successful!\n",
      "Inserted data into [volcano].[summary]\n",
      "Connection successful!\n",
      "Inserted data into [volcano].[affected_areas]\n",
      "Connection successful!\n",
      "Inserted data into [volcano].[timeline]\n",
      "Connection successful!\n",
      "Inserted data into [volcano].[impact]\n",
      "Connection successful!\n",
      "Inserted data into [volcano].[provinces]\n",
      "Connection successful!\n",
      "Inserted data into [volcano].[cities]\n",
      "Connection successful!\n",
      "Inserted data into [volcano].[airports]\n",
      "Connection successful!\n",
      "Inserted data into [volcano].[ports]\n",
      "Connection successful!\n",
      "Inserted data into [volcano].[dams]\n",
      "Connection successful!\n",
      "Inserted data into [volcano].[nuclear_plants]\n",
      "Connection successful!\n",
      "Inserted data into [wildfire].[summary]\n",
      "Connection successful!\n",
      "Inserted data into [wildfire].[impact]\n",
      "Connection successful!\n",
      "Inserted data into [wildfire].[timeline]\n",
      "Connection successful!\n",
      "Inserted data into [wildfire].[provinces]\n",
      "Connection successful!\n",
      "Inserted data into [wildfire].[cities]\n",
      "Connection successful!\n",
      "Inserted data into [wildfire].[airports]\n",
      "Connection successful!\n",
      "Inserted data into [wildfire].[ports]\n",
      "Connection successful!\n",
      "Inserted data into [wildfire].[dams]\n",
      "Connection successful!\n",
      "Inserted data into [wildfire].[nuclear_plants]\n"
     ]
    }
   ],
   "source": [
    "target_dir_name = 'active_dir'\n",
    "absolute_folder_path = Path(target_dir_name).resolve()\n",
    "filenames = list_filenames(absolute_folder_path)\n",
    "for file in filenames:\n",
    "    absolute_file_path = absolute_folder_path / file\n",
    "    load_into_table(absolute_file_path,filenames_dict.get(file))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
