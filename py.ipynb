{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGDACS ID WF 1020968\\nCountries Australia\\nEvent summary This forest fire is expected to have a low humanitarian impact based on the magnitude and the affected population and their vulnerability.\\nGDACS ID\\tWF 1020968\\nCountries:\\tAustralia\\nStart Date - Last detection*:\\t09 Aug 2024 - 14 Aug 2024\\nDuration (days):\\t5\\nPeople affected:\\t0 in the burned area\\nBurned area:\\t10556 ha\\n\\n\\nDynamically store the cols and pass the Xpath into a list element\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "GDACS ID WF 1020968\n",
    "Countries Australia\n",
    "Event summary This forest fire is expected to have a low humanitarian impact based on the magnitude and the affected population and their vulnerability.\n",
    "GDACS ID\tWF 1020968\n",
    "Countries:\tAustralia\n",
    "Start Date - Last detection*:\t09 Aug 2024 - 14 Aug 2024\n",
    "Duration (days):\t5\n",
    "People affected:\t0 in the burned area\n",
    "Burned area:\t10556 ha\n",
    "\n",
    "\n",
    "Dynamically store the cols and pass the Xpath into a list element\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Dataframes\n",
    "# Main table\n",
    "table_0_headers = ['Event_id','Episode','Event_type','Impact_url']\n",
    "table_1_headers = [\"Event_id\",\"Episode\",\"Countries\",\"Start_date_last_detected\",\"Duration\",\"People_affected\",\"Burned_area\",\"Event_summary\"]\n",
    "table_2_headers = ['Event_id','Episode','ID','Alert_Color','GDACS_Score','Population_Affected','Burned_Area','Last_Update','GWIS']\n",
    "table_3_headers = ['Event_id','Episode','Radius','Population']\n",
    "table_4_headers = ['Event_id','Episode','Region_province','Country','Population']\n",
    "table_5_headers = ['Event_id','Episode','Name','Region_Province','Country','City_class','Population','Distance']\n",
    "table_6_headers = ['Event_id','Episode','Name','IATA_Code','Elevation_in_m','Usage','Runway_type','IFR','Runway_Length_in_ft','Distance']\n",
    "table_7_headers = ['Event_id','Episode','Name','LOCODE','Country','Distance']\n",
    "table_8_headers = ['Event_id','Episode','Reservoir','Dam_Name','River','Year','Distance']\n",
    "table_9_headers = ['Event_id','Episode','Name','Country','Reactor','Distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004452 10 Wildfires\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "html_file = \"file:///D:/1004452_10_Wildfires_WF.html\"\n",
    "# https://www.gdacs.org/Wildfires/report.aspx?eventtype=WF&eventid=1004452&episodeid=10\n",
    "# \"file:///F:/Web Scraping/latest_htmls/Wildfires/1021646_9_Wildfires_WF.html\"\n",
    "# \"file:///F:/Web Scraping/htmls/Wildfires/1.html\"\n",
    "# \"https://www.gdacs.org/Wildfires/report.aspx?eventtype=WF&eventid=1019635&episodeid=2\"\n",
    "# Open the URL\n",
    "file_name = html_file.split('/')[-1].split('.')[0]\n",
    "event_id,episode,event_type = file_name.split('_')[0:3]\n",
    "\n",
    "\n",
    "print(event_id,episode,event_type)\n",
    "# event_type = file_name[1].split('/')[1]\n",
    "# episode = file_name[2]\n",
    "# print(file_name,event_id,event_type,episode)\n",
    "driver.get(html_file)\n",
    "\n",
    "# event_id = '1021461'\n",
    "# event_type = 'wildfires'\n",
    "# episode = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004452', '10', 'Republic of Korea', '04 Mar 2022 - 12 Mar 2022', '8', '1580 in the burned area', '19748 ha', 'This forest fire is expected to have a low humanitarian impact based on the magnitude and the affected population and their vulnerability.']\n"
     ]
    }
   ],
   "source": [
    "# Table 1\n",
    "# Fetch the event summary section\n",
    "try:\n",
    "    event_path = '//*[@class=\"p_summary\"][1]'\n",
    "    event_summary_element = driver.find_element(By.XPATH, event_path)\n",
    "    event_summary_text = event_summary_element.text\n",
    "except NoSuchElementException:\n",
    "    print(f\"Table with the provided XPath '{event_path}' not found.\")\n",
    "table_1_df =[]\n",
    "try:\n",
    "    table_xpath = '//*[@id=\"alert_summary_left\"]/table/tbody'  # Adjust XPath as needed\n",
    "    table = driver.find_element(By.XPATH, table_xpath)\n",
    "    # Fetch table rows using XPath\n",
    "    rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "    rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "    # Extract data from each row and column\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "        columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "        row_data = [column.text for column in columns]\n",
    "        if len(row_data) <1:\n",
    "            continue\n",
    "        else:\n",
    "            table_data.append(row_data)\n",
    "\n",
    "    table_1_df = [\n",
    "        event_id,\n",
    "        episode,\n",
    "        *[table_data[i][1] for i in range(1, 6)],\n",
    "        event_summary_text\n",
    "    ]\n",
    "    print(table_1_df)\n",
    "except NoSuchElementException:\n",
    "    print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004452', '10', 'Republic of Korea', '04 Mar 2022 - 12 Mar 2022', '8', '1580 in the burned area', '19748 ha', 'This forest fire is expected to have a low humanitarian impact based on the magnitude and the affected population and their vulnerability.']\n"
     ]
    }
   ],
   "source": [
    "table_1_df =[]\n",
    "try:\n",
    "    table_xpath = '//*[@id=\"alert_summary_left\"]/table/tbody'  # Adjust XPath as needed\n",
    "    table = driver.find_element(By.XPATH, table_xpath)\n",
    "    # Fetch table rows using XPath\n",
    "    rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "    rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "    # Extract data from each row and column\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "        columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "        row_data = [column.text for column in columns]\n",
    "        if len(row_data) <1:\n",
    "            continue\n",
    "        else:\n",
    "            table_data.append(row_data)\n",
    "\n",
    "    table_1_df = [\n",
    "        event_id,\n",
    "        episode,\n",
    "        *[table_data[i][1] for i in range(1, 6)],\n",
    "        event_summary_text\n",
    "    ]\n",
    "    print(table_1_df)\n",
    "except NoSuchElementException:\n",
    "    print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004452', '10', '1', '', '0.5', '1044', '7992', '05 Mar 2022 04:01', 'GWIS']\n",
      "['1004452', '10', '2', '', '0.5', '1044', '8053', '05 Mar 2022 11:37', 'GWIS']\n",
      "['1004452', '10', '3', '', '0.5', '1539', '12682', '06 Mar 2022 04:01', 'GWIS']\n",
      "['1004452', '10', '4', '', '0.5', '1570', '15222', '07 Mar 2022 04:01', 'GWIS']\n",
      "['1004452', '10', '5', '', '0.5', '1580', '17363', '08 Mar 2022 04:01', 'GWIS']\n",
      "['1004452', '10', '6', '', '0.5', '1570', '18099', '09 Mar 2022 04:01', 'GWIS']\n",
      "['1004452', '10', '7', '', '0.5', '1580', '18435', '10 Mar 2022 04:00', 'GWIS']\n",
      "['1004452', '10', '8', '', '0.5', '1580', '19387', '11 Mar 2022 04:00', 'GWIS']\n",
      "['1004452', '10', '9', '', '0.5', '1633', '19728', '12 Mar 2022 04:00', 'GWIS']\n",
      "['1004452', '10', '10', '', '0.5', '1580', '19748', '13 Mar 2022 04:00', 'GWIS']\n"
     ]
    }
   ],
   "source": [
    "# table 2\n",
    "table_2_df = []\n",
    "try:\n",
    "    # Locate the Impact Timeline\n",
    "    table_xpath = '//*[@id=\"ctl00_CPH_GridViewEpisodes\"]/tbody'  # Adjust XPath as needed\n",
    "    table = driver.find_element(By.XPATH, table_xpath)\n",
    "    # Fetch table rows using XPath\n",
    "    rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "    rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "    # Extract data from each row and column\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "        columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "        row_data = [column.text for column in columns]\n",
    "        if len(row_data) <1:\n",
    "            continue\n",
    "        else:\n",
    "            table_data.append(row_data)\n",
    "    for i in range(len(table_data)):\n",
    "        row = [\n",
    "            event_id,\n",
    "            episode,\n",
    "            *table_data[i][:7] \n",
    "        ]\n",
    "        table_2_df.append(row)\n",
    "\n",
    "    for row in table_2_df:\n",
    "        print(row)\n",
    "except NoSuchElementException:\n",
    "    print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004452', '10', 'Radius', 'Population']\n",
      "['1004452', '10', '10 km', '32000 people']\n",
      "['1004452', '10', '5 km', '27000 people']\n",
      "['1004452', '10', '2 km', '20000 people']\n",
      "['1004452', '10', '1 km', '11000 people']\n",
      "['1004452', '10', 'Burned Area', '1600 people']\n"
     ]
    }
   ],
   "source": [
    "table_3_df = []\n",
    "try:\n",
    "    # Locate the Exposed population\n",
    "    table_xpath = '//*[@id=\"graph_eq\"]/table/tbody/tr/td/table/tbody'  # Adjust XPath as needed\n",
    "    table = driver.find_element(By.XPATH, table_xpath)\n",
    "    # Fetch table rows using XPath\n",
    "    rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "    rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "    # Extract data from each row and column\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        columns_xpath = './/th | .//td'  # XPath to locate columns within each row\n",
    "        columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "        row_data = [column.text for column in columns]\n",
    "        if len(row_data) <1:\n",
    "            continue\n",
    "        else:\n",
    "            table_data.append(row_data)\n",
    "\n",
    "    for i in range(len(table_data)):\n",
    "        row = [\n",
    "            event_id,\n",
    "            episode,\n",
    "            *table_data[i][:2] \n",
    "        ]\n",
    "        table_3_df.append(row)\n",
    "\n",
    "    for row in table_3_df:\n",
    "        print(row)\n",
    "except NoSuchElementException:\n",
    "    print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004452', '10', 'Kyongsang-bukto', 'Republic of Korea', '2.9 million people']\n",
      "['1004452', '10', 'Kangwon-do', 'Republic of Korea', '1.6 million people']\n"
     ]
    }
   ],
   "source": [
    "# Table 4\n",
    "table_4_df = []\n",
    "try:\n",
    "    # Locate the Affected Provinces\n",
    "    table_xpath = '//*[@id=\"provinces\"]/table/tbody'  # Adjust XPath as needed\n",
    "    table = driver.find_element(By.XPATH, table_xpath)\n",
    "    # Fetch table rows using XPath\n",
    "    rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "    rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "    # Extract data from each row and column\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "        columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "        row_data = [column.text for column in columns]\n",
    "        if len(row_data) <1:\n",
    "            continue\n",
    "        else:\n",
    "            table_data.append(row_data)\n",
    "\n",
    "        \n",
    "    for i in range(len(table_data)):\n",
    "        row = [\n",
    "            event_id,\n",
    "            episode,\n",
    "            *table_data[i][:3] \n",
    "        ]\n",
    "        table_4_df.append(row)\n",
    "\n",
    "    for row in table_4_df:\n",
    "        print(row)\n",
    "except NoSuchElementException:\n",
    "    print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait = WebDriverWait(driver, 10)\n",
    "# btn = wait.until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"cookie-consent-banner\"]/div/div/div[2]/a[1]')))\n",
    "# btn.click()\n",
    "# scroll_script = \"window.scrollTo(0, document.body.scrollHeight * 0.7);\"\n",
    "# driver.execute_script(scroll_script) \n",
    "# time.sleep(10)\n",
    "# btn = wait.until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"airports\"]/div/div[1]')))\n",
    "# btn.click()\n",
    "# btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004452', '10', 'Yeongweol', 'Kang-Won-Do', 'Republic of Korea', 'City', '26000 people', '74 km']\n",
      "['1004452', '10', 'Andong', 'Kyongsangbuk-Do', 'Republic of Korea', 'City', '130000 people', '75 km']\n",
      "['1004452', '10', 'Gangneung', 'Kang-Won-Do', 'Republic of Korea', 'City', '-', '85 km']\n",
      "['1004452', '10', 'Yecheon', 'Kyongsangbuk-Do', 'Republic of Korea', 'City', '-', '89 km']\n"
     ]
    }
   ],
   "source": [
    "table_5_df = []\n",
    "try:\n",
    "    # Locate the Affected populated places\n",
    "    table_xpath = '//*[@id=\"cities\"]/table/tbody'  # Adjust XPath as needed\n",
    "    table = driver.find_element(By.XPATH, table_xpath)\n",
    "    # Fetch table rows using XPath\n",
    "    rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "    rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "    # Extract data from each row and column\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "        columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "        row_data = [column.text.strip() for column in columns]\n",
    "        # row_data = [column.text for column in columns]\n",
    "        if len(row_data) <1:\n",
    "            continue\n",
    "        else:\n",
    "            table_data.append(row_data)\n",
    "        \n",
    "        \n",
    "    for i in range(len(table_data)):\n",
    "        row = [\n",
    "            event_id,\n",
    "            episode,\n",
    "            *table_data[i][:6] \n",
    "        ]\n",
    "        table_5_df.append(row)\n",
    "\n",
    "    for row in table_5_df:\n",
    "        print(row)\n",
    "except NoSuchElementException:\n",
    "    print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004452', '10', 'R 430', '', '2', '', '', '', '0', '39 km']\n",
      "['1004452', '10', 'Samchok', 'SUK', 'unknown', '', '', '', '0', '46 km']\n",
      "['1004452', '10', 'Gangneung', 'KAG', '11', 'Mil.', 'Paved', 'Yes', '8900', '83 km']\n",
      "['1004452', '10', 'R 418', '', '283', '', '', '', '0', '87 km']\n",
      "['1004452', '10', 'R 417', '', '525', '', '', '', '0', '92 km']\n"
     ]
    }
   ],
   "source": [
    "table_6_df = []\n",
    "try:\n",
    "    # Table 6\n",
    "    # Locate the airports\n",
    "    table_xpath = '//*[@id=\"airports\"]/table/tbody'  # Adjust XPath as needed\n",
    "    table = driver.find_element(By.XPATH, table_xpath)\n",
    "    # Fetch table rows using XPath\n",
    "    rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "    rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "    # Extract data from each row and column\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "        columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "        row_data = [column.text.strip() for column in columns]\n",
    "        # row_data = [column.text for column in columns]\n",
    "        if len(row_data) <1:\n",
    "            continue\n",
    "        else:\n",
    "            table_data.append(row_data)\n",
    "        \n",
    "        \n",
    "    for i in range(len(table_data)):\n",
    "        row = [\n",
    "            event_id,\n",
    "            episode,\n",
    "            *table_data[i][:8] \n",
    "        ]\n",
    "        table_6_df.append(row)\n",
    "\n",
    "    for row in table_6_df:\n",
    "        print(row)\n",
    "except NoSuchElementException:\n",
    "    print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004452', '10', 'Ulchin', '', 'South Korea', '14 km']\n",
      "['1004452', '10', 'Tonghae', 'KRTGH', 'South Korea', '50 km']\n",
      "['1004452', '10', 'Mukho', 'KRMUK', 'South Korea', '56 km']\n"
     ]
    }
   ],
   "source": [
    "# Table 7\n",
    "# Locate the ports\n",
    "table_7_df = []\n",
    "try:\n",
    "    table_xpath = '//*[@id=\"ports\"]/table/tbody'  # Adjust XPath as needed\n",
    "    table = driver.find_element(By.XPATH, table_xpath)\n",
    "    # Fetch table rows using XPath\n",
    "    rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "    rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "    # Extract data from each row and column\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "        columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "        row_data = [column.text.strip() for column in columns]\n",
    "        # row_data = [column.text for column in columns]\n",
    "        if len(row_data) <1:\n",
    "            continue\n",
    "        else:\n",
    "            table_data.append(row_data)\n",
    "        \n",
    "        \n",
    "    for i in range(len(table_data)):\n",
    "        row = [\n",
    "            event_id,\n",
    "            episode,\n",
    "            *table_data[i][:4] \n",
    "        ]\n",
    "        table_7_df.append(row)\n",
    "\n",
    "    for row in table_7_df:\n",
    "        print(row)\n",
    "        \n",
    "except NoSuchElementException:\n",
    "    print(f\"Table with the provided XPath '{table_xpath}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004452', '10', '', 'Imha', 'Panbyon', '1993', '69 km']\n",
      "['1004452', '10', '', 'An Dong', 'Nakdong', '1977', '71 km']\n",
      "['1004452', '10', '', 'Toam', 'Song', 'n/a', '80 km']\n"
     ]
    }
   ],
   "source": [
    "table_8_df = []\n",
    "# Table 8\n",
    "# Locate the dams\n",
    "try:\n",
    "    table_xpath = '//*[@id=\"dams\"]/table/tbody'  # Adjust XPath as needed\n",
    "    table = driver.find_element(By.XPATH, table_xpath)\n",
    "    # Fetch table rows using XPath\n",
    "    rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "    rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "    # Extract data from each row and column\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "        columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "        row_data = [column.text.strip() for column in columns]\n",
    "        # row_data = [column.text for column in columns]\n",
    "        if len(row_data) <1:\n",
    "            continue\n",
    "        else:\n",
    "            table_data.append(row_data)\n",
    "        \n",
    "    for i in range(len(table_data)):\n",
    "        row = [\n",
    "            event_id,\n",
    "            episode,\n",
    "            *table_data[i][:5] \n",
    "        ]\n",
    "        table_8_df.append(row)\n",
    "\n",
    "    for row in table_8_df:\n",
    "        print(row)\n",
    "        \n",
    "except NoSuchElementException:\n",
    "    print(f\"Table with the provided XPath '{table_xpath}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004452', '10', 'ULCHIN', 'REPUBLIC OF KOREA', '6', '8 km']\n"
     ]
    }
   ],
   "source": [
    "table_9_df = []\n",
    "# Table 9\n",
    "# Locate the nuclear plants\n",
    "try:\n",
    "    table_xpath = '//*[@id=\"nuclear\"]/table/tbody'  # Adjust XPath as needed\n",
    "    table = driver.find_element(By.XPATH, table_xpath)\n",
    "    # Fetch table rows using XPath\n",
    "    rows_xpath = './/tr'  # XPath to locate rows within the table\n",
    "    rows = table.find_elements(By.XPATH, rows_xpath)\n",
    "    # Extract data from each row and column\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        columns_xpath = './/td'  # XPath to locate columns within each row\n",
    "        columns = row.find_elements(By.XPATH, columns_xpath)\n",
    "        row_data = [column.text.strip() for column in columns]\n",
    "        # row_data = [column.text for column in columns]\n",
    "        if len(row_data) <1:\n",
    "            continue\n",
    "        else:\n",
    "            table_data.append(row_data)\n",
    "        \n",
    "    for i in range(len(table_data)):\n",
    "        row = [\n",
    "            event_id,\n",
    "            episode,\n",
    "            *table_data[i][:5] \n",
    "        ]\n",
    "        table_9_df.append(row)\n",
    "\n",
    "    for row in table_9_df:\n",
    "        print(row)\n",
    "        \n",
    "except NoSuchElementException:\n",
    "    print(f\"Table with the provided XPath '{table_xpath}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for row in table_2_df:\n",
    "    print(len(row))\n",
    "print(len(table_2_headers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (3) does not match length of index (7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m tbl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_df\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtbl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m      7\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m destination_folder\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\CZ0222\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:754\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    745\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    746\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    747\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    748\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m         dtype,\n\u001b[0;32m    753\u001b[0m     )\n\u001b[1;32m--> 754\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    763\u001b[0m         data,\n\u001b[0;32m    764\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    768\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    769\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\CZ0222\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:123\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\CZ0222\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:620\u001b[0m, in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    615\u001b[0m             val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m    617\u001b[0m         val \u001b[38;5;241m=\u001b[39m sanitize_array(\n\u001b[0;32m    618\u001b[0m             val, index, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, raise_cast_failure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    619\u001b[0m         )\n\u001b[1;32m--> 620\u001b[0m         \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m     homogenized\u001b[38;5;241m.\u001b[39mappend(val)\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m homogenized\n",
      "File \u001b[1;32mc:\\Users\\CZ0222\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (3) does not match length of index (7)"
     ]
    }
   ],
   "source": [
    "destination_folder = 'D:\\Web Scraping\\Web-Scraping\\CSV\\Wildfires\\\\'\n",
    "i=8\n",
    "col = eval(f\"table_{i}_headers\")\n",
    "tbl = eval(f'table_{i}_df')\n",
    "df = pd.DataFrame(tbl,col)\n",
    "df.head()\n",
    "csv_file_path = destination_folder+f'{i}.csv'\n",
    "# Check if the file exists\n",
    "file_exists = os.path.exists(csv_file_path)\n",
    "    # Append to the file\n",
    "\n",
    "# Write or append to the CSV file\n",
    "# df.to_csv(csv_file_path, mode= 'w', header= True, index=False)\n",
    "df.to_csv(csv_file_path, mode='a' if file_exists else 'w', header=not file_exists, index=False)\n",
    "print(f\"Data {'appended to' if file_exists else 'written to'} {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended to D:\\Web Scraping\\Web-Scraping\\CSV\\Wildfires\\1.csv\n"
     ]
    }
   ],
   "source": [
    "df =pd.DataFrame([['1004452', '10', 'ULCHIN', 'REPUBLIC OF KOREA', '6', '8 km']],columns=['Event_id', 'Episode', 'Name', 'Country', 'Reactor', 'Distance'])\n",
    "csv_file_path = destination_folder+f'{i}.csv'\n",
    "# Check if the file exists\n",
    "file_exists = os.path.exists(csv_file_path)\n",
    "    # Append to the file\n",
    "\n",
    "# Write or append to the CSV file\n",
    "# df.to_csv(csv_file_path, mode= 'w', header= True, index=False)\n",
    "df.to_csv(csv_file_path, mode='a' if file_exists else 'w', header=not file_exists, index=False)\n",
    "print(f\"Data {'appended to' if file_exists else 'written to'} {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  0\n",
      "Event_id                                    1004452\n",
      "Episode                                          10\n",
      "Countries                         Republic of Korea\n",
      "Start_date_last_detected  04 Mar 2022 - 12 Mar 2022\n",
      "Duration                                          8\n",
      "Data written to D:\\Web Scraping\\Web-Scraping\\CSV\\Wildfires\\1.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (10) does not match length of index (9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[179], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m tbl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_df\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtbl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m      8\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m destination_folder\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\CZ0222\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:754\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    745\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    746\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    747\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    748\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m         dtype,\n\u001b[0;32m    753\u001b[0m     )\n\u001b[1;32m--> 754\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    763\u001b[0m         data,\n\u001b[0;32m    764\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    768\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    769\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\CZ0222\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:123\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\CZ0222\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:620\u001b[0m, in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    615\u001b[0m             val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m    617\u001b[0m         val \u001b[38;5;241m=\u001b[39m sanitize_array(\n\u001b[0;32m    618\u001b[0m             val, index, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, raise_cast_failure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    619\u001b[0m         )\n\u001b[1;32m--> 620\u001b[0m         \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m     homogenized\u001b[38;5;241m.\u001b[39mappend(val)\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m homogenized\n",
      "File \u001b[1;32mc:\\Users\\CZ0222\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (10) does not match length of index (9)"
     ]
    }
   ],
   "source": [
    "# Generate CSV Files\n",
    "destination_folder = 'D:\\Web Scraping\\Web-Scraping\\CSV\\Wildfires\\\\'\n",
    "for i in range(1,10):\n",
    "    header = eval(f\"table_{i}_headers\")\n",
    "    tbl = eval(f'table_{i}_df')\n",
    "    df = pd.DataFrame(tbl,header)\n",
    "    print(df.head())\n",
    "    csv_file_path = destination_folder+f'{i}.csv'\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.exists(csv_file_path)\n",
    "        # Append to the file\n",
    "    \n",
    "    # Write or append to the CSV file\n",
    "    # df.to_csv(csv_file_path, mode= 'w', header= True, index=False)\n",
    "    df.to_csv(csv_file_path, mode='a' if file_exists else 'w', header=not file_exists, index=False)\n",
    "    print(f\"Data {'appended to' if file_exists else 'written to'} {csv_file_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2673710196.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[190], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    <table>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<table>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
